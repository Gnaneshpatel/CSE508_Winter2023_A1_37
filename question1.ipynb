{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q1(1) CONVERTING CONTENTS OF FILE INTO PROPER FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3aYZFg2r8gDw"
      },
      "outputs": [],
      "source": [
        "# IMPORTING ALL FILES INTO A LITS\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the list of all files\n",
        "path = \"D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset\"\n",
        "dir_list = os.listdir(path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EXTRACTING THE REQUIRED CONTENT AND WRITING IT BACK INTO THE FILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        data = file.read().replace('\\n', ' ')\n",
        "    s = \"<TITLE>(.*?)</TITLE>\"\n",
        "    r = re.findall(s, data)\n",
        "    str(r)\n",
        "    s1 = \"<TEXT>(.*?)</TEXT>\"\n",
        "    r1 = re.findall(s1, data)\n",
        "    str(r1)\n",
        "\n",
        "\n",
        "    with open(x, 'w') as file:\n",
        "        pass\n",
        "        for s in r:\n",
        "            file.write(s)\n",
        "        for s1 in r1:\n",
        "            file.write(s1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-> PREPROCESSING STEPS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) CONVERTING CONTENTS OF ALL THE FILES TO LOWER CASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        for line in file:\n",
        "            data = line.lower()\n",
        "    \n",
        "    with open(x, 'w') as file:\n",
        "        pass\n",
        "        for s in data:\n",
        "            file.write(s)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IMPORTING ALL THE NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) REMOVING ALL THE PUNCTUATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove punctuation\n",
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        for line in file:\n",
        "          translator = str.maketrans('', '', string.punctuation)\n",
        "          data=line.translate(translator)\n",
        "    \n",
        "    with open(x, 'w') as file:\n",
        "      pass\n",
        "      for s in data:\n",
        "        file.write(s) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        for line in file:\n",
        "            word_tokenize(line)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) REMOVING BLANK SPACES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        for line in file:\n",
        "          data=\" \".join(line.split())\n",
        "\n",
        "    with open(x, 'w') as file:\n",
        "      pass\n",
        "      for s in data:\n",
        "        file.write(s)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5) REMOVING STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def remove(data):\n",
        "\tstop_words = set(stopwords.words(\"english\"))\n",
        "\ttokens = word_tokenize(data)\n",
        "\twithout_stopwords = [w for w in tokens if w not in stop_words]\n",
        "\treturn without_stopwords\n",
        "\n",
        "for file in dir_list:\n",
        "    x='D://ir//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
        "    with open(x, 'r') as file:\n",
        "        for line in file:\n",
        "          data=remove(line)\n",
        "    with open(x, 'w') as file:\n",
        "      pass\n",
        "      for s in data:\n",
        "        file.write(s + ' ')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
