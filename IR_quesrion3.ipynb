{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OS module\n",
    "import os\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = 'C://Users//ECOO33AU//OneDrive//Desktop//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'\n",
    "dir_list = os.listdir(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3(1) BIGRAM INVERTED INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def build_bigram_model(filenames):\n",
    "    bigram_model = {}\n",
    "\n",
    "    for filename in filenames:\n",
    "        str = filename\n",
    "        filename='C://Users//ECOO33AU//OneDrive//Desktop//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+filename\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "            words = text.split()\n",
    "\n",
    "            for i in range(len(words) - 1):\n",
    "                # taking the first word\n",
    "                first_word = words[i]\n",
    "                # taking the adjacent word along with it to form bigram\n",
    "                second_word = words[i + 1]\n",
    "\n",
    "                bigram = (first_word, second_word)\n",
    "                # checking if the bigram is present in our bigram_model or not\n",
    "                if bigram_model.get(bigram) is None:\n",
    "                    # if not present than append it\n",
    "                    bigram_model[bigram]=[str]\n",
    "                else:\n",
    "                    # else iterating in the bigram_model and finding it out\n",
    "                    for w in bigram_model.get(bigram):\n",
    "                        if str not in bigram_model.get(bigram):\n",
    "                            bigram_model[bigram].append(str)\n",
    "\n",
    "    return bigram_model\n",
    "\n",
    "# storing the result in bigram_model\n",
    "bigram_model = build_bigram_model(dir_list)\n",
    "\n",
    "# print(bigram_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING THE BIGRAM MODEL USING PICKLE MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# using pickle module saving the result in bigram_model.pkl\n",
    "with open('bigram_model.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_model, f)\n",
    "    # print(bigram_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3(2) POSITIONAL INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def positional(files):\n",
    "    positional_model = defaultdict(list)\n",
    "    for file in files: \n",
    "        x='C://Users//ECOO33AU//OneDrive//Desktop//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'+file\n",
    "        with open(x, 'r') as f:\n",
    "            text = f.read().split()\n",
    "            # finding the positions of the particular words in each document\n",
    "            for i, word in enumerate(text):\n",
    "                # append the result in position_model\n",
    "                positional_model[word].append({file: i})\n",
    "    return dict(positional_model)\n",
    "\n",
    "# storing the result in position_list\n",
    "position_list=positional(dir_list)\n",
    "# print(position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the frequencies of the words appearing in several docs\n",
    "um = (positional(dir_list))\n",
    "positional_answer=[]\n",
    "for key in um:\n",
    "    temp=[]\n",
    "    temp.append(key)\n",
    "    temp.append(len(um[key]))\n",
    "    temp.append(um[key])\n",
    "    # print(len(um[key]))\n",
    "    positional_answer.append(temp)\n",
    "    \n",
    "# print(positional_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING POSITIONALINDEX USING PICKLE MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# using pickle module saving the result in position_list.pkl\n",
    "with open('position_list.pkl', 'wb') as f:\n",
    "    pickle.dump(position_list, f)\n",
    "    # print(position_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3(3) COMPARING AND COMMENTING THE RESULTS FOR Q3(1) AND Q3(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) TAKING INPUT FROM USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking number of testcases as user input\n",
    "x = int(input())\n",
    "data= []\n",
    "count=0\n",
    "\n",
    "while(x>0):\n",
    "    y = input()\n",
    "    if(len(y.split())<=5):\n",
    "        x=x-1\n",
    "        # storing the user input in list named data\n",
    "        data.append(y)\n",
    "    else:\n",
    "        # returning error if the statement length is >5\n",
    "        print(\"Enter input of length <=5 \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING THE INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ECOO33AU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ECOO33AU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING TO LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heating rate may occur', 'similar solutions compressible']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i.lower() for i in data]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING PUNCTUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heating rate may occur', 'similar solutions compressible']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data=[i.translate(translator) for i in data]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['heating', 'rate', 'may', 'occur'], ['similar', 'solutions', 'compressible']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "\tstop_words = set(stopwords.words(\"english\"))\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tfiltered_text = [word for word in word_tokens if word not in stop_words]\n",
    "\treturn filtered_text\n",
    "\n",
    "data1=[]\n",
    "for i in data:\n",
    "    data1.append(remove_stopwords(i))\n",
    "    \n",
    "data1\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRAMING THE COMPLETE SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heating rate may occur ', 'similar solutions compressible ']\n"
     ]
    }
   ],
   "source": [
    "data2=[]\n",
    "\n",
    "for i in data1:\n",
    "    s=\"\"\n",
    "    for j in i:\n",
    "        s+=(j + ' ')\n",
    "    data2.append(s)\n",
    "        \n",
    "print(data2)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING BLANK SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heating rate may occur', 'similar solutions compressible']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\" \".join(i.split()) for i in data2]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR EXTRACTING BIGRAM INVERTED INDEX FOR GIVEN INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def fun1(data):\n",
    "    \n",
    "    # building the bigram_model for our user inputs\n",
    "    bigram_model1 = defaultdict(list)\n",
    "    text = data.split()\n",
    "    for m in range(len(text)-1):\n",
    "        first_word = text[m]\n",
    "        second_word = text[m + 1]\n",
    "        # print(first_word)\n",
    "\n",
    "        bigram = (first_word, second_word)\n",
    "        if bigram_model1.get(bigram) is None:\n",
    "            bigram_model1[bigram]\n",
    "                # print(l)\n",
    "        else:\n",
    "            for w in bigram_model1.get(bigram):\n",
    "                if str not in bigram_model1.get(bigram):\n",
    "                    bigram_model1[bigram].append()\n",
    "\n",
    "    # from the list of bigrams of user inputchecking if they are present in our actual bigram model\n",
    "    ans=[]\n",
    "    flag = False\n",
    "    for i in bigram_model1:\n",
    "        if i in bigram_model:\n",
    "            flag =True\n",
    "            # if present than append it in the list\n",
    "            ans.append(bigram_model[i])\n",
    "        \n",
    "    if(flag):\n",
    "        i=0\n",
    "        while(i<len(ans)-1):\n",
    "            x=set(ans[i])\n",
    "            y=set(ans[i+1])\n",
    "            # finding the intersection of two lists of bigrams\n",
    "            z=x.intersection(y)\n",
    "            # storing the result in i+1 position so that next time it can be compared with the i+2 input\n",
    "            ans[i+1]=z\n",
    "            i=i+1\n",
    "    else:\n",
    "        print(\"not present\")\n",
    "        \n",
    "    return ans[len(ans)-1], len(ans[len(ans)-1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR EXTRACTING POSITIONAL INDEX FOR GIVEN INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun2(data):\n",
    "    # building the position index for the given user input\n",
    "    ans=[]\n",
    "    ans1=[]\n",
    "    for j in data.split():\n",
    "        if j in position_list:\n",
    "            ans.append(position_list[j])\n",
    "        else:\n",
    "            print(\"not present\")\n",
    "    ans1.append(ans)\n",
    "    \n",
    "    # storing the list of each word in a list\n",
    "    set_list=[]\n",
    "    for ans in ans1:\n",
    "        for i in range(len(ans)): \n",
    "            set1=[]\n",
    "            for j in range(len(ans[i])):\n",
    "                # print(j)\n",
    "                for k in ans[i][j]:\n",
    "                    set1.append(k)\n",
    "            set_list.append(set1)\n",
    "        \n",
    "    \n",
    "    set1=[]\n",
    "    for i in set_list:\n",
    "        set1.append(i)\n",
    "\n",
    "    # taking the intersection of each list in set_list\n",
    "    for i in range(len(set_list)-1):\n",
    "        x=set(set_list[i])\n",
    "        y=set(set_list[i+1])\n",
    "        set_list[i+1]=x.intersection(y)\n",
    "    \n",
    "    \n",
    "    answer=set_list[len(set_list)-1]\n",
    "    \n",
    "    # final documents list after finding the intersection between each list\n",
    "    final_list=[]\n",
    "    for x in answer:\n",
    "        for i in range(len(ans)):\n",
    "            for j in range(len(ans[i])):\n",
    "                if x in ans[i][j]:\n",
    "                    final_list.append(ans[i][j])\n",
    "                \n",
    "    check=[]\n",
    "    final=[]\n",
    "    for j in range(len(final_list)):\n",
    "        for i in final_list[j]:\n",
    "            for p in i:\n",
    "                if final_list[j][i] not in check:\n",
    "                    # storing the poitions(values) in a separate list named check\n",
    "                    check.append(final_list[j][i])\n",
    "        # sorting the values list\n",
    "        check.sort()\n",
    "       \n",
    "        k = len(data.split())\n",
    "        \n",
    "        for i in range(len(check)-k+1):\n",
    "            flag=0\n",
    "            flag1=0\n",
    "            # checking for the diff between the positions of first and last word\n",
    "            if(check[i+(k-1)]-check[i] == k-1):\n",
    "                # if we get the desired difference than retrieve the document names from final list \n",
    "                for a in final_list:\n",
    "                    for p in a:\n",
    "                        if(a[p]==check[i]):\n",
    "                            \n",
    "                            temp=a.keys()\n",
    "                            flag=1\n",
    "                        if(a[p]==check[i+(k-1)]):\n",
    "                            temp1=a.keys()\n",
    "                            flag1=1\n",
    "                        if flag==1 and flag1==1:\n",
    "                            if(temp == temp1):  \n",
    "                                flag=0\n",
    "                                flag1=0\n",
    "                                # if the retrieved docs for both positions are same than append it\n",
    "                                if temp not in final:\n",
    "                                    final.append(temp)\n",
    "            \n",
    "    return final,len(final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrived for query 1 using bigram index is : 1\n",
      "Names of documents retrived for query 1 using bigram index is : {'cranfield0005'}\n",
      "Number of documents retrived for query 1 using positional index is : 1\n",
      "Names of documents retrived for query 1 using positional index is : [dict_keys(['cranfield0005'])]\n",
      "Number of documents retrived for query 2 using bigram index is : 4\n",
      "Names of documents retrived for query 2 using bigram index is : {'cranfield0565', 'cranfield0062', 'cranfield0011', 'cranfield0305'}\n",
      "Number of documents retrived for query 2 using positional index is : 4\n",
      "Names of documents retrived for query 2 using positional index is : [dict_keys(['cranfield0305']), dict_keys(['cranfield0565']), dict_keys(['cranfield0011']), dict_keys(['cranfield0062'])]\n"
     ]
    }
   ],
   "source": [
    "count=1\n",
    "\n",
    "for i in data:\n",
    "    print(\"Number of documents retrived for query {} using bigram index is :\".format(count),fun1(i)[1])\n",
    "    print(\"Names of documents retrived for query {} using bigram index is :\".format(count),fun1(i)[0])\n",
    "    print(\"Number of documents retrived for query {} using positional index is :\".format(count),fun2(i)[1])\n",
    "    print(\"Names of documents retrived for query {} using positional index is :\".format(count),fun2(i)[0])\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b43dddb97dd0035d8d45d3db5e4e252852c7d746c9115980eacd8ad3cdd2123"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
